{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZR92LB2aPpf"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone --single-branch -b colab-train https://github.com/gamerzahatv/tacotron2-update-python3.10.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tacotron2-update-python3.10\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySXKkY6jatBl",
        "outputId": "ba17bcfa-9c76-42ed-db9c-c0545ae01a79"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2-update-python3.10\n",
            "/content/tacotron2-update-python3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/tacotron2_train_data/dang_dataset_test_03_08_2020.tar /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwdsxyjjbHCG",
        "outputId": "268ccd96-a765-4e54-b79b-2f43277e23c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "tar: Cowardly refusing to create an empty archive\n",
            "Try 'tar --help' or 'tar --usage' for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dang_dataset\n",
        "!tar -xvf /content/dang_dataset_test_03_08_2020.tar    -C /content\n",
        "!cp -r /content/dang_dataset_test_03_08_2020/dang_03_08_2020  /content/tacotron2-update-python3.10/dang_dataset\n",
        "!cp -r /content/dang_dataset_test_03_08_2020/filelist/* /content/tacotron2-update-python3.10/filelists"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_AVUxwHfvrN",
        "outputId": "ae663cbf-bd04-4f15-ac6e-bea1617bc2db"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dang_dataset’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "READ THIS DOC https://github.com/NVIDIA/tacotron2/blob/master/README.md"
      ],
      "metadata": {
        "id": "SEJtpKJOgvkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        " `python train.py --output_directory=outdir --log_directory=logdir`\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "# (OPTIONAL)\n",
        "`tensorboard --logdir=outdir/logdir`\n",
        "\n",
        "-------------------------------------------\n",
        "# Training using a pre-trained model\n",
        "Training using a pre-trained model can lead to faster convergence\n",
        "\n",
        "By default, the dataset dependent text embedding layers are ignored\n",
        "\n",
        "Download our published Tacotron 2 model\n",
        "\n",
        "`python train.py --output_directory=outdir --log_directory=logdir -c tacotron2_statedict.pt --warm_start`"
      ],
      "metadata": {
        "id": "ysum1aqVhAQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/tacotron2-update-python3.10/requirements_3.10.txt"
      ],
      "metadata": {
        "id": "drko3uktjtmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tacotron2-update-python3.10\n",
        "!python train.py --output_directory=/content/drive/MyDrive/tacotron2_train_data --log_directory=/content/drive/MyDrive/tacotron2_train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r8HJiPkiaNc",
        "outputId": "d385d260-84fa-4380-9507-74c3ec1fe018"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2-update-python3.10\n",
            "2024-11-20 12:51:11.937022: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-20 12:51:11.937088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-20 12:51:11.939070: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-20 12:51:11.950839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-20 12:51:13.172391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "FP16 Run: False\n",
            "Dynamic Loss Scaling: True\n",
            "Distributed Run: True\n",
            "cuDNN Enabled: True\n",
            "cuDNN Benchmark: True\n",
            "Using GPU: Tesla T4\n",
            "Initializing Distributed\n",
            "/content/tacotron2-update-python3.10/stft.py:67: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
            "  fft_window = pad_center(fft_window, filter_length)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch: 0\n",
            "/content/tacotron2-update-python3.10/utils.py:8: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
            "/content/tacotron2-update-python3.10/train.py:23: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
            "Train loss 0 22.875006 Grad Norm 8.434945 8.70s/it\n",
            "Validation loss 0: 15.260956  \n",
            "Saving model and optimizer state at iteration 0 to /content/drive/MyDrive/tacotron2_train_data/checkpoint_0\n",
            "Model Saved\n",
            "Train loss 1 11.521101 Grad Norm 17.368046 4.60s/it\n",
            "Train loss 2 7.660215 Grad Norm 7.199834 5.12s/it\n",
            "Train loss 3 3.793653 Grad Norm 5.208764 7.48s/it\n",
            "Train loss 4 3.363124 Grad Norm 4.024357 4.30s/it\n",
            "Train loss 5 2.982575 Grad Norm 53.006721 3.97s/it\n",
            "Train loss 6 2.381058 Grad Norm 7.021372 5.89s/it\n",
            "Train loss 7 3.594724 Grad Norm 72.342781 4.30s/it\n",
            "Train loss 8 2.580170 Grad Norm 35.467594 6.19s/it\n",
            "Train loss 9 2.536029 Grad Norm 31.919962 5.81s/it\n",
            "Train loss 10 3.928664 Grad Norm 9.144386 4.14s/it\n",
            "Train loss 11 2.832632 Grad Norm 8.334230 5.23s/it\n",
            "Train loss 12 2.574703 Grad Norm 6.293249 6.02s/it\n",
            "Train loss 13 3.121424 Grad Norm 30.856939 5.87s/it\n",
            "Train loss 14 2.152705 Grad Norm 27.455889 6.07s/it\n",
            "Train loss 15 3.129784 Grad Norm 35.021313 4.27s/it\n",
            "Train loss 16 2.249288 Grad Norm 43.881523 5.55s/it\n",
            "Train loss 17 2.553188 Grad Norm 18.123886 5.48s/it\n",
            "Train loss 18 3.692342 Grad Norm 97.237839 3.44s/it\n",
            "Train loss 19 3.399480 Grad Norm 99.332161 3.96s/it\n",
            "Train loss 20 3.530373 Grad Norm 85.552704 2.98s/it\n",
            "Train loss 21 2.919992 Grad Norm 36.416626 3.63s/it\n",
            "Train loss 22 4.001107 Grad Norm 52.040699 4.10s/it\n",
            "Train loss 23 3.243809 Grad Norm 52.648724 4.42s/it\n",
            "Train loss 24 2.180152 Grad Norm 44.034573 4.60s/it\n",
            "Train loss 25 2.109472 Grad Norm 4.204116 6.29s/it\n",
            "Train loss 26 2.277753 Grad Norm 13.246309 4.42s/it\n",
            "Train loss 27 2.396775 Grad Norm 9.403282 2.56s/it\n",
            "Epoch: 1\n",
            "Train loss 28 2.591444 Grad Norm 2.237003 4.62s/it\n",
            "Train loss 29 1.847438 Grad Norm 11.347636 3.08s/it\n",
            "Train loss 30 2.992205 Grad Norm 32.843025 3.10s/it\n",
            "Train loss 31 2.129599 Grad Norm 28.161238 2.79s/it\n",
            "Train loss 32 2.316467 Grad Norm 23.323229 2.48s/it\n",
            "Train loss 33 2.292087 Grad Norm 3.176322 2.22s/it\n",
            "Train loss 34 1.608247 Grad Norm 21.315453 3.80s/it\n",
            "Train loss 35 2.197942 Grad Norm 4.774030 2.65s/it\n",
            "Train loss 36 1.588820 Grad Norm 19.994270 3.70s/it\n",
            "Train loss 37 1.636130 Grad Norm 23.269020 3.28s/it\n",
            "Train loss 38 2.362221 Grad Norm 2.419950 1.95s/it\n",
            "Train loss 39 1.850823 Grad Norm 11.339062 3.14s/it\n",
            "Train loss 40 1.834287 Grad Norm 17.559793 4.02s/it\n",
            "Train loss 41 2.481474 Grad Norm 10.525860 3.49s/it\n",
            "Train loss 42 1.838853 Grad Norm 18.909613 3.13s/it\n",
            "Train loss 43 2.647013 Grad Norm 16.519081 2.36s/it\n",
            "Train loss 44 1.892355 Grad Norm 11.895541 3.52s/it\n",
            "Train loss 45 2.223104 Grad Norm 12.027400 3.31s/it\n",
            "Train loss 46 2.842874 Grad Norm 14.441337 2.39s/it\n",
            "Train loss 47 2.157388 Grad Norm 8.780925 2.50s/it\n",
            "Train loss 48 2.362962 Grad Norm 10.989326 1.44s/it\n",
            "Train loss 49 2.225096 Grad Norm 14.553044 1.73s/it\n",
            "Train loss 50 3.280345 Grad Norm 14.784794 1.95s/it\n",
            "Train loss 51 2.467088 Grad Norm 4.361887 2.98s/it\n",
            "Train loss 52 1.658447 Grad Norm 3.151680 6.36s/it\n",
            "Train loss 53 1.918491 Grad Norm 20.233147 3.35s/it\n",
            "Train loss 54 2.004103 Grad Norm 10.125177 2.51s/it\n",
            "Train loss 55 2.153888 Grad Norm 6.035757 1.50s/it\n",
            "Epoch: 2\n",
            "Train loss 56 2.399705 Grad Norm 10.891091 3.47s/it\n",
            "Train loss 57 1.754265 Grad Norm 12.469265 2.97s/it\n",
            "Train loss 58 2.582127 Grad Norm 13.252937 3.57s/it\n",
            "Train loss 59 1.691280 Grad Norm 4.278291 2.84s/it\n",
            "Train loss 60 1.923551 Grad Norm 7.608592 2.41s/it\n",
            "Train loss 61 1.862172 Grad Norm 4.611705 2.26s/it\n",
            "Train loss 62 1.315296 Grad Norm 0.650104 3.38s/it\n",
            "Train loss 63 1.971620 Grad Norm 10.545631 2.70s/it\n",
            "Train loss 64 1.357286 Grad Norm 7.706247 4.24s/it\n",
            "Train loss 65 1.339271 Grad Norm 3.623703 3.34s/it\n",
            "Train loss 66 2.176765 Grad Norm 5.057627 1.99s/it\n",
            "Train loss 67 1.734793 Grad Norm 9.073999 2.83s/it\n",
            "Train loss 68 1.608666 Grad Norm 7.641968 3.45s/it\n",
            "Train loss 69 2.248528 Grad Norm 9.850864 3.41s/it\n",
            "Train loss 70 1.571839 Grad Norm 7.609889 3.48s/it\n",
            "Train loss 71 2.342229 Grad Norm 8.361637 2.40s/it\n",
            "Train loss 72 1.491466 Grad Norm 1.114071 4.90s/it\n",
            "Train loss 73 1.768627 Grad Norm 2.780059 3.40s/it\n",
            "Train loss 74 2.201291 Grad Norm 2.734696 2.42s/it\n",
            "Train loss 75 1.722642 Grad Norm 3.764256 2.62s/it\n",
            "Train loss 76 1.827372 Grad Norm 2.605161 1.44s/it\n",
            "Train loss 77 1.746713 Grad Norm 2.660772 1.71s/it\n",
            "Train loss 78 2.593303 Grad Norm 5.947370 1.97s/it\n",
            "Train loss 79 1.892580 Grad Norm 4.942814 2.33s/it\n",
            "Train loss 80 1.270166 Grad Norm 1.350088 2.96s/it\n",
            "Train loss 81 1.306043 Grad Norm 1.112053 3.40s/it\n",
            "Train loss 82 1.538894 Grad Norm 4.350483 3.01s/it\n",
            "Train loss 83 1.789551 Grad Norm 5.369898 1.50s/it\n",
            "Epoch: 3\n",
            "Train loss 84 1.813959 Grad Norm 5.051350 4.12s/it\n",
            "Train loss 85 1.252559 Grad Norm 1.838361 2.62s/it\n",
            "Train loss 86 1.867869 Grad Norm 7.943491 3.84s/it\n",
            "Train loss 87 1.406898 Grad Norm 6.903047 3.79s/it\n",
            "Train loss 88 1.444281 Grad Norm 1.892855 2.47s/it\n",
            "Train loss 89 1.456907 Grad Norm 3.627321 2.39s/it\n",
            "Train loss 90 1.041605 Grad Norm 2.971287 3.26s/it\n",
            "Train loss 91 1.527242 Grad Norm 5.314116 2.24s/it\n",
            "Train loss 92 0.944789 Grad Norm 2.235680 4.17s/it\n",
            "Train loss 93 0.938583 Grad Norm 2.339638 3.88s/it\n",
            "Train loss 94 1.555463 Grad Norm 4.830610 2.01s/it\n",
            "Train loss 95 1.145419 Grad Norm 2.487074 2.82s/it\n",
            "Train loss 96 1.098921 Grad Norm 4.724427 3.46s/it\n",
            "Train loss 97 1.607972 Grad Norm 6.465091 3.38s/it\n",
            "Train loss 98 0.998317 Grad Norm 2.554433 3.74s/it\n",
            "Train loss 99 1.569189 Grad Norm 7.266422 2.39s/it\n",
            "Train loss 100 1.133503 Grad Norm 3.957170 3.89s/it\n",
            "Validation loss 100:  2.619723  \n",
            "Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/tacotron2_train_data/checkpoint_100\n",
            "Model Saved\n",
            "Train loss 101 1.241359 Grad Norm 1.602574 2.89s/it\n",
            "Train loss 102 1.647925 Grad Norm 3.716661 1.89s/it\n",
            "Train loss 103 1.239926 Grad Norm 3.896596 2.30s/it\n",
            "Train loss 104 1.536175 Grad Norm 10.125187 3.59s/it\n",
            "Train loss 105 1.431999 Grad Norm 8.379934 2.98s/it\n",
            "Train loss 106 1.855955 Grad Norm 7.566310 1.94s/it\n",
            "Train loss 107 1.528553 Grad Norm 4.548160 2.71s/it\n",
            "Train loss 108 0.906008 Grad Norm 1.212891 3.65s/it\n",
            "Train loss 109 0.986849 Grad Norm 2.515624 3.23s/it\n",
            "Train loss 110 1.102052 Grad Norm 2.451552 2.84s/it\n",
            "Train loss 111 1.444870 Grad Norm 5.470642 1.81s/it\n",
            "Epoch: 4\n",
            "Train loss 112 1.310297 Grad Norm 3.322539 3.66s/it\n",
            "Train loss 113 1.066173 Grad Norm 5.962405 2.60s/it\n",
            "Train loss 114 1.302122 Grad Norm 2.956839 3.03s/it\n",
            "Train loss 115 1.094120 Grad Norm 3.014331 3.42s/it\n",
            "Train loss 116 1.098582 Grad Norm 3.033348 2.98s/it\n",
            "Train loss 117 1.291742 Grad Norm 7.022002 2.23s/it\n",
            "Train loss 118 0.778854 Grad Norm 1.021311 3.51s/it\n",
            "Train loss 119 1.195878 Grad Norm 3.265795 2.24s/it\n",
            "Train loss 120 0.784756 Grad Norm 1.327346 3.56s/it\n",
            "Train loss 121 1.019974 Grad Norm 4.528953 3.94s/it\n",
            "Train loss 122 1.733830 Grad Norm 10.263058 2.51s/it\n",
            "Train loss 123 1.006678 Grad Norm 4.366705 3.03s/it\n",
            "Train loss 124 0.967503 Grad Norm 3.401495 3.40s/it\n",
            "Train loss 125 1.299621 Grad Norm 3.643275 2.92s/it\n",
            "Train loss 126 0.843789 Grad Norm 1.129111 4.97s/it\n",
            "Train loss 127 1.165084 Grad Norm 2.740055 2.95s/it\n",
            "Train loss 128 0.870074 Grad Norm 1.534412 3.90s/it\n",
            "Train loss 129 1.069186 Grad Norm 1.910723 2.87s/it\n",
            "Train loss 130 1.462487 Grad Norm 2.521264 1.90s/it\n",
            "Train loss 131 1.135258 Grad Norm 2.574506 2.28s/it\n",
            "Train loss 132 1.311252 Grad Norm 3.757824 1.72s/it\n",
            "Train loss 133 1.270223 Grad Norm 2.548624 2.01s/it\n",
            "Train loss 134 1.496742 Grad Norm 3.122024 2.23s/it\n",
            "Train loss 135 1.244274 Grad Norm 2.445285 2.30s/it\n",
            "Train loss 136 0.813088 Grad Norm 1.121897 2.93s/it\n",
            "Train loss 137 0.899576 Grad Norm 0.592825 3.00s/it\n",
            "Train loss 138 0.961960 Grad Norm 1.066448 2.53s/it\n",
            "Train loss 139 1.287703 Grad Norm 1.953857 1.75s/it\n",
            "Epoch: 5\n",
            "Train loss 140 1.197797 Grad Norm 1.120854 3.57s/it\n",
            "Train loss 141 0.899320 Grad Norm 1.309144 2.61s/it\n",
            "Train loss 142 1.186743 Grad Norm 3.513793 2.98s/it\n",
            "Train loss 143 1.084392 Grad Norm 2.537903 3.89s/it\n",
            "Train loss 144 1.114551 Grad Norm 2.362543 5.18s/it\n",
            "Train loss 145 1.181199 Grad Norm 2.854976 2.24s/it\n",
            "Train loss 146 0.813070 Grad Norm 1.291833 3.26s/it\n",
            "Train loss 147 1.083959 Grad Norm 1.436710 2.25s/it\n",
            "Train loss 148 0.734006 Grad Norm 1.336758 3.86s/it\n",
            "Train loss 149 0.786223 Grad Norm 1.292827 3.87s/it\n",
            "Train loss 150 1.236895 Grad Norm 3.175293 1.96s/it\n",
            "Train loss 151 0.819483 Grad Norm 1.072448 2.83s/it\n",
            "Train loss 152 0.851780 Grad Norm 3.425051 3.47s/it\n",
            "Train loss 153 1.213890 Grad Norm 4.429087 2.92s/it\n",
            "Train loss 154 0.859834 Grad Norm 1.168189 3.75s/it\n",
            "Train loss 155 1.072684 Grad Norm 1.143456 3.00s/it\n",
            "Train loss 156 0.830904 Grad Norm 1.859862 3.52s/it\n",
            "Train loss 157 1.112901 Grad Norm 2.517419 2.97s/it\n",
            "Train loss 158 1.472995 Grad Norm 3.473921 1.91s/it\n",
            "Train loss 159 1.152540 Grad Norm 2.290260 2.66s/it\n",
            "Train loss 160 1.254489 Grad Norm 1.789556 1.71s/it\n",
            "Train loss 161 1.242211 Grad Norm 1.980110 1.98s/it\n",
            "Train loss 162 1.456458 Grad Norm 2.572215 2.23s/it\n",
            "Train loss 163 1.189141 Grad Norm 2.626306 2.31s/it\n",
            "Train loss 164 0.806654 Grad Norm 0.959953 2.93s/it\n",
            "Train loss 165 0.889649 Grad Norm 1.211605 2.96s/it\n",
            "Train loss 166 0.943206 Grad Norm 1.425649 2.49s/it\n",
            "Train loss 167 1.245522 Grad Norm 1.106948 1.79s/it\n",
            "Epoch: 6\n",
            "Train loss 168 1.165315 Grad Norm 1.461233 4.35s/it\n",
            "Train loss 169 0.989450 Grad Norm 2.939468 2.60s/it\n",
            "Train loss 170 1.152139 Grad Norm 2.940095 2.98s/it\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15\n",
        "!tensorboard==2.15\n",
        "\n"
      ],
      "metadata": {
        "id": "e_kM1LO0mCVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's|\\\\|/|g' /content/tacotron2-update-python3.10/filelists/train.txt\n",
        "!sed -i 's|\\\\|/|g' /content/tacotron2-update-python3.10/filelists/test.txt"
      ],
      "metadata": {
        "id": "sBRLCsisnktl"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}